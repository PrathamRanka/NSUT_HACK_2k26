# -*- coding: utf-8 -*-
"""ML_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bUn1eKdvV_RIeKqf1NT9fKlGcD7Oq21k

#Dataset Feature Extraction and Libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import IsolationForest

import tensorflow as tf
from tensorflow.keras import layers, losses
from tensorflow.keras import models

from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn
import os
import json

df = pd.read_csv('/content/government-procurement-via-gebiz.csv')

df

df.tail()

df.shape

"""#EDA"""

df.describe()

df.info()

df.isnull().sum()

df['supplier_name'].value_counts()

sns.boxplot(x=df["awarded_amt"])
plt.show()

"""#Preprocessing"""

df["award_date"] = pd.to_datetime(df["award_date"], errors="coerce")

#Removing Negatives
df = df[df["awarded_amt"] > 0]

#Filling Missing Values
df["supplier_name"] = df["supplier_name"].fillna("UNKNOWN")
df["agency"] = df["agency"].fillna("UNKNOWN")

"""Contract value features"""

df["log_amount"] = np.log1p(df["awarded_amt"])

"""Supplier Risk Features"""

supplier_stats = df.groupby("supplier_name")["awarded_amt"].agg(["mean", "count"]).reset_index()
supplier_stats.columns = ["supplier_name", "supplier_avg_amt", "supplier_contract_count"]

df = df.merge(supplier_stats, on="supplier_name", how="left")

"""Agency Risk Features"""

agency_stats = df.groupby("agency")["awarded_amt"].agg(["mean", "count"]).reset_index()
agency_stats.columns = ["agency", "agency_avg_amt", "agency_contract_count"]

df = df.merge(agency_stats, on="agency", how="left")

"""Time Features"""

df["year"] = df["award_date"].dt.year
df["month"] = df["award_date"].dt.month
df["day"] = df["award_date"].dt.day

features = [
    "awarded_amt",
    "log_amount",
    "supplier_avg_amt",
    "supplier_contract_count",
    "agency_avg_amt",
    "agency_contract_count",
    "year",
    "month"
]

X = df[features].fillna(0)

X.head()

plt.figure(figsize=(6,6))
sns.lineplot(x= X['agency_avg_amt'],y=X['supplier_avg_amt'])

"""#Model Training

Data Scaling
"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

if_model = IsolationForest(
    n_estimators = 300,
    contamination='auto',
    random_state=42
)

if_model.fit(X_scaled)

df["if_score"] = -if_model.score_samples(X_scaled)

df["if_pred"] = if_model.predict(X_scaled)

"""#AutoEncoder"""

X_scaled

input_dim = X_scaled.shape[1]

input_layer = layers.Input(shape=(input_dim,))

"""Encoder"""

encoded = layers.Dense(32, activation="relu")(input_layer)
encoded = layers.Dense(16, activation="relu")(encoded)

"""Decoder"""

decoded = layers.Dense(32, activation="relu")(encoded)
decoded = layers.Dense(input_dim, activation=None)(decoded)

ae_model = models.Model(input_layer, decoded)

ae_model.compile(
    optimizer="adam",
    loss="mse"
)
autoencoder.summary()

"""AE Training"""

history = ae_model.fit(
    X_scaled, X_scaled,
    epochs=30,
    batch_size=64,
    shuffle=True,
    validation_split=0.1,
    verbose=1
)

"""Error Calc"""

reconstructions = ae_model.predict(X_scaled)

reconstruction_error = np.mean(
    np.square(X_scaled - reconstructions), axis=1
)

df["ae_score"] = reconstruction_error

"""#Hybrid Score Calculation"""

scaler_mm = MinMaxScaler()

df[["if_norm", "ae_norm"]] = scaler_mm.fit_transform(
    df[["if_score", "ae_score"]]
)

df["fraud_score"] = (
    0.6 * df["if_norm"] +
    0.4 * df["ae_norm"]
)

def assign_risk(score):
    if score >= 0.8:
        return "HIGH"
    elif score >= 0.5:
        return "MEDIUM"
    else:
        return "LOW"

df["risk_level"] = df["fraud_score"].apply(risk_label)

df.head()

df.tail()

"""#Generating Reasons"""

def generate_reasons(row):
    reasons = []

    agency_std = row.get("agency_std", 0)
    agency_avg = row.get("agency_avg_amt", 0)

    # Agency Z-score anomaly
    if pd.notna(agency_std) and agency_std > 0:
        z = (row["awarded_amt"] - agency_avg) / agency_std
        if z > 3:
            reasons.append(f"Amount is {z:.1f}x deviations above agency average")

    # Supplier favoritism
    if row.get("supplier_contract_count", 0) > 10:
        reasons.append("Supplier has unusually high number of contracts")

    # Extreme contract value
    if agency_avg > 0 and row["awarded_amt"] > agency_avg * 5:
        reasons.append("Contract value far exceeds agency norm")

    # End-of-year budget dumping
    if row.get("month") in [11, 12, 1, 2, 3]:
        reasons.append("Contract awarded near financial year end")

    # Round number fraud
    if row["awarded_amt"] > 10000 and row["awarded_amt"] % 1000 == 0:
        reasons.append("Suspiciously round contract amount")

    if len(reasons) == 0:
        reasons.append("Detected unusual procurement behavior")

    return reasons

import math

def safe_int(x):
    if pd.isna(x) or math.isnan(x):
        return None
    return int(x)

def generate_fraud_report(row):
    return {
        "supplier_name": row.get("supplier_name", "Unknown"),
        "agency": row.get("agency", "Unknown"),
        "awarded_amt": float(row["awarded_amt"]),
        "fraud_score": round(float(row["fraud_score"]), 3),
        "risk_level": row["risk_level"],
        "year": safe_int(row.get("year")),
        "month": safe_int(row.get("month")),
        "reasons": generate_reasons(row)
    }

alerts_df = df.sort_values("fraud_score", ascending=False).head(20)
fraud_reports = alerts_df.apply(generate_fraud_report, axis=1).tolist()
fraud_reports[:3]

"""#Json Conversion"""

with open("fraud_reports.json", "w") as f:
    json.dump(fraud_reports, f, indent=4)